{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d158446e5c14263adfabd2d1a109c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_594e3884085e42c39d3da17ddd009a6e",
              "IPY_MODEL_4358b756ef874c4aa6ea53f1647bf223",
              "IPY_MODEL_ae6cdc5efb15465fa8e77efd52ab4192"
            ],
            "layout": "IPY_MODEL_ba94dae472a246c3848251687e3a1cbd"
          }
        },
        "594e3884085e42c39d3da17ddd009a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bc100e0561a4310bcc6b3ba47654631",
            "placeholder": "​",
            "style": "IPY_MODEL_c1d3dc3c4b294045a1455621008d5896",
            "value": "yolox_l0.05.onnx: 100%"
          }
        },
        "4358b756ef874c4aa6ea53f1647bf223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5efa1dd2841c4c2a9e0563fd9ae882cf",
            "max": 216625723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1ae700c5e3d4d5b93bb3c276a8dba3a",
            "value": 216625723
          }
        },
        "ae6cdc5efb15465fa8e77efd52ab4192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64ff8d849fab4e03b08134a5fa2ea658",
            "placeholder": "​",
            "style": "IPY_MODEL_a66e5b4a098d4c52bffcc476bc549711",
            "value": " 217M/217M [00:02&lt;00:00, 90.0MB/s]"
          }
        },
        "ba94dae472a246c3848251687e3a1cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bc100e0561a4310bcc6b3ba47654631": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1d3dc3c4b294045a1455621008d5896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5efa1dd2841c4c2a9e0563fd9ae882cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1ae700c5e3d4d5b93bb3c276a8dba3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64ff8d849fab4e03b08134a5fa2ea658": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a66e5b4a098d4c52bffcc476bc549711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D2_wDAb9Z80t",
        "outputId": "255e4663-3995-46b0-c4da-cfb1bbedfcb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured[all-docs]\n",
            "  Downloading unstructured-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Collecting filetype (from unstructured[all-docs])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured[all-docs])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.3)\n",
            "Collecting emoji (from unstructured[all-docs])\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from unstructured[all-docs])\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting python-iso639 (from unstructured[all-docs])\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured[all-docs])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.25.2)\n",
            "Collecting rapidfuzz (from unstructured[all-docs])\n",
            "  Downloading rapidfuzz-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured[all-docs])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured[all-docs])\n",
            "  Downloading unstructured_client-0.24.1-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.9.5)\n",
            "Collecting pillow-heif (from unstructured[all-docs])\n",
            "  Downloading pillow_heif-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.5)\n",
            "Collecting google-cloud-vision (from unstructured[all-docs])\n",
            "  Downloading google_cloud_vision-3.7.3-py2.py3-none-any.whl (466 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.4/466.4 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured-inference==0.7.36 (from unstructured[all-docs])\n",
            "  Downloading unstructured_inference-0.7.36-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx>=1.1.2 (from unstructured[all-docs])\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pikepdf (from unstructured[all-docs])\n",
            "  Downloading pikepdf-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Collecting python-pptx<=0.6.23 (from unstructured[all-docs])\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting effdet (from unstructured[all-docs])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypandoc (from unstructured[all-docs])\n",
            "  Downloading pypandoc-1.13-py3-none-any.whl (21 kB)\n",
            "Collecting pypdf (from unstructured[all-docs])\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract (from unstructured[all-docs])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting onnx (from unstructured[all-docs])\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-oxmsg (from unstructured[all-docs])\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.6)\n",
            "Collecting pdf2image (from unstructured[all-docs])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
            "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.3)\n",
            "Collecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting layoutparser (from unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.23.5)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.8.0.76)\n",
            "Collecting onnxruntime>=1.17.0 (from unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (2.3.1+cu121)\n",
            "Collecting timm (from unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.42.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.20.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.23->unstructured[all-docs])\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.5)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured[all-docs])\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured[all-docs])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (0.18.1+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet->unstructured[all-docs])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.16.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (3.20.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (2024.5.15)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (42.0.8)\n",
            "Collecting pillow\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pikepdf->unstructured[all-docs])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting olefile (from python-oxmsg->unstructured[all-docs])\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2024.7.4)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.27.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
            "Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.16.0)\n",
            "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured[all-docs])\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.63.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx>=0.27.0->unstructured-client->unstructured[all-docs])\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[all-docs])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[all-docs])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.1)\n",
            "Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.13.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.7.36->unstructured[all-docs]) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (3.15.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36->unstructured[all-docs]) (0.19.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (1.11.4)\n",
            "Collecting iopath (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading pdfplumber-0.11.2-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.1.5)\n",
            "Collecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.3.0)\n",
            "Building wheels for collected packages: langdetect, antlr4-python3-runtime, iopath\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=b86774b6ea492466b667188cd0bf7c2b31cbf00a313075c5fa8a0f5933dc929e\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=0fc1034fc55d12f59939a2e64873d64a43ad3ae675d009fa6a2c322a4cc16f43\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=c49613376fbf8c62d99d26ef6f9ed57520d6ecbc0786a9fe5605a12cea19d160\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built langdetect antlr4-python3-runtime iopath\n",
            "Installing collected packages: filetype, antlr4-python3-runtime, XlsxWriter, rapidfuzz, python-multipart, python-magic, python-iso639, python-docx, pypdfium2, pypdf, pypandoc, portalocker, pillow, ordered-set, onnx, omegaconf, olefile, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, langdetect, jsonpath-python, humanfriendly, h11, emoji, Deprecated, backoff, unstructured.pytesseract, typing-inspect, requests-toolbelt, python-pptx, python-oxmsg, pytesseract, pillow-heif, pikepdf, pdf2image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, httpcore, deepdiff, coloredlogs, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, httpx, dataclasses-json, unstructured-client, pdfplumber, unstructured, layoutparser, google-cloud-vision, timm, unstructured-inference, effdet\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 XlsxWriter-3.2.0 antlr4-python3-runtime-4.9.3 backoff-2.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 deepdiff-7.0.1 effdet-0.4.1 emoji-2.12.1 filetype-1.2.0 google-cloud-vision-3.7.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 humanfriendly-10.0 iopath-0.1.10 jsonpath-python-1.0.6 langdetect-1.0.9 layoutparser-0.3.4 marshmallow-3.21.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 olefile-0.47 omegaconf-2.3.0 onnx-1.16.1 onnxruntime-1.18.1 ordered-set-4.1.0 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.2 pikepdf-9.0.0 pillow-10.4.0 pillow-heif-0.17.0 portalocker-2.10.1 pypandoc-1.13 pypdf-4.3.1 pypdfium2-4.30.0 pytesseract-0.3.10 python-docx-1.1.2 python-iso639-2024.4.27 python-magic-0.4.27 python-multipart-0.0.9 python-oxmsg-0.0.1 python-pptx-0.6.23 rapidfuzz-3.9.4 requests-toolbelt-1.0.0 timm-1.0.7 typing-inspect-0.9.0 unstructured-0.15.0 unstructured-client-0.24.1 unstructured-inference-0.7.36 unstructured.pytesseract-0.3.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "da8961abe1db41ad9010604c4b5a4b64"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"unstructured[all-docs]\" pillow pydantic lxml matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_uyuw7oaTWg",
        "outputId": "1f25bbd6-6272-4cb0-87f9-fe49b7fba08c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 14.2 kB/129 kB 11%] [Connected to cloud.r\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 46.0 kB/129 kB 36%] [Connected to cloud.r\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,707 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [859 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,128 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,064 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,418 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,781 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,329 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.7 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,192 kB]\n",
            "Get:22 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,545 kB]\n",
            "Fetched 24.5 MB in 3s (7,607 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0eJ_1oa7GA",
        "outputId": "ed411500-39ae-46e9-818e-534d1cbe378c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.4 [186 kB]\n",
            "Fetched 186 kB in 1s (303 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123586 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MciZd1LUbLM0",
        "outputId": "cf65d36d-6c13-4664-baa9-ef693d7961ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libimagequant0 libraqm0 mailcap mime-support python3-olefile\n",
            "  tesseract-ocr-osd\n",
            "Suggested packages:\n",
            "  python-pil-doc\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libimagequant0 libleptonica-dev libraqm0 libtesseract-dev\n",
            "  mailcap mime-support python3-olefile python3-pil tesseract-ocr\n",
            "  tesseract-ocr-eng tesseract-ocr-osd tesseract-ocr-script-latn\n",
            "0 upgraded, 13 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 40.0 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.1 [582 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libimagequant0 amd64 2.17.0-1 [34.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libraqm0 amd64 0.7.0-4ubuntu1 [11.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-olefile all 0.46-3 [33.8 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-pil amd64 9.0.1-1ubuntu0.3 [419 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-latn all 1:4.00~git30-7274cfa-1.1 [30.9 MB]\n",
            "Fetched 40.0 MB in 2s (17.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 13.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 123616 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libarchive-dev_3.6.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libimagequant0:amd64.\n",
            "Preparing to unpack .../01-libimagequant0_2.17.0-1_amd64.deb ...\n",
            "Unpacking libimagequant0:amd64 (2.17.0-1) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../02-libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libraqm0:amd64.\n",
            "Preparing to unpack .../03-libraqm0_0.7.0-4ubuntu1_amd64.deb ...\n",
            "Unpacking libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../04-libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../05-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../06-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package python3-olefile.\n",
            "Preparing to unpack .../07-python3-olefile_0.46-3_all.deb ...\n",
            "Unpacking python3-olefile (0.46-3) ...\n",
            "Selecting previously unselected package python3-pil:amd64.\n",
            "Preparing to unpack .../08-python3-pil_9.0.1-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking python3-pil:amd64 (9.0.1-1ubuntu0.3) ...\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "Preparing to unpack .../09-tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../10-tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../11-tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-latn.\n",
            "Preparing to unpack .../12-tesseract-ocr-script-latn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up python3-olefile (0.46-3) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Setting up libimagequant0:amd64 (2.17.0-1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up python3-pil:amd64 (9.0.1-1ubuntu0.3) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured-pytesseract\n",
        "!pip install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8og0aG_Qb5oB",
        "outputId": "4d2583b4-d36d-45e6-ce95-f5b59a7634e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured-pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured-pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-pytesseract) (10.4.0)\n",
            "Collecting tesseract-ocr\n",
            "  Downloading tesseract-ocr-0.0.1.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from tesseract-ocr) (3.0.10)\n",
            "Building wheels for collected packages: tesseract-ocr\n",
            "  Building wheel for tesseract-ocr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesseract-ocr: filename=tesseract_ocr-0.0.1-cp310-cp310-linux_x86_64.whl size=169756 sha256=25bc9bf5afd494bea711273c582e37edc0b6518ea3a2837e32676219bd38ce01\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/fd/f3/5c231ecbbb80a1fe33204ff3021d99b54ef6daf6f8099311b8\n",
            "Successfully built tesseract-ocr\n",
            "Installing collected packages: tesseract-ocr\n",
            "Successfully installed tesseract-ocr-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf"
      ],
      "metadata": {
        "id": "ulv5tpbkcExA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the object of the unstructured partition pdf\n",
        "raw_pdf_elements=partition_pdf(\n",
        "    filename=\"/content/data/ULM.pdf\",\n",
        "    strategy=\"hi_res\",\n",
        "    extract_images_in_pdf=True,\n",
        "    extract_image_block_types=[\"Image\", \"Table\"],\n",
        "    extract_image_block_to_payload=False,\n",
        "    extract_image_block_output_dir=\"extracted_data\"\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0d158446e5c14263adfabd2d1a109c23",
            "594e3884085e42c39d3da17ddd009a6e",
            "4358b756ef874c4aa6ea53f1647bf223",
            "ae6cdc5efb15465fa8e77efd52ab4192",
            "ba94dae472a246c3848251687e3a1cbd",
            "7bc100e0561a4310bcc6b3ba47654631",
            "c1d3dc3c4b294045a1455621008d5896",
            "5efa1dd2841c4c2a9e0563fd9ae882cf",
            "f1ae700c5e3d4d5b93bb3c276a8dba3a",
            "64ff8d849fab4e03b08134a5fa2ea658",
            "a66e5b4a098d4c52bffcc476bc549711"
          ]
        },
        "id": "QW9KYrZXcaUr",
        "outputId": "4c844720-cae6-468a-d8db-b4f7fde6f08d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "yolox_l0.05.onnx:   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d158446e5c14263adfabd2d1a109c23"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpOntTPIeNi-",
        "outputId": "7338f4d7-9f93-412b-82ae-a7dec4a59198"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.Text at 0x7db768f40f70>,\n",
              " <unstructured.documents.elements.Header at 0x7db768d804f0>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f41990>,\n",
              " <unstructured.documents.elements.Title at 0x7db768d832b0>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f41ff0>,\n",
              " <unstructured.documents.elements.Title at 0x7db768d80a90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768d80eb0>,\n",
              " <unstructured.documents.elements.Title at 0x7db768d83310>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768d81570>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768d81330>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768d80460>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f41b40>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768d81000>,\n",
              " <unstructured.documents.elements.Table at 0x7db768d80610>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f42590>,\n",
              " <unstructured.documents.elements.Table at 0x7db76b486740>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f41960>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f41b70>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b486fe0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b487430>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b486dd0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b486e00>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b487670>,\n",
              " <unstructured.documents.elements.Title at 0x7db76b484760>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8d8d0>,\n",
              " <unstructured.documents.elements.Footer at 0x7db768c8d660>,\n",
              " <unstructured.documents.elements.Image at 0x7db768c8d810>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7db768c8d0f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8d360>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8d720>,\n",
              " <unstructured.documents.elements.Title at 0x7db768c8d000>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8d300>,\n",
              " <unstructured.documents.elements.Title at 0x7db768c8d510>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8c940>,\n",
              " <unstructured.documents.elements.Footer at 0x7db768c8f070>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f40fa0>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f400a0>,\n",
              " <unstructured.documents.elements.Formula at 0x7db768c8efb0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f40a60>,\n",
              " <unstructured.documents.elements.Formula at 0x7db768c8f310>,\n",
              " <unstructured.documents.elements.Formula at 0x7db768c8c070>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8c0a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8e890>,\n",
              " <unstructured.documents.elements.Title at 0x7db768c8e6b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8e860>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8c550>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8e500>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8e2f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8cfd0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8c8e0>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f40250>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8da80>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8de10>,\n",
              " <unstructured.documents.elements.Title at 0x7db768c8ffa0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8f4c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8f4f0>,\n",
              " <unstructured.documents.elements.Title at 0x7db768c8d3f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8f1f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8e620>,\n",
              " <unstructured.documents.elements.Title at 0x7db768c8c3d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8c400>,\n",
              " <unstructured.documents.elements.Title at 0x7db768c8c340>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768c8f100>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f41600>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f40b20>,\n",
              " <unstructured.documents.elements.Table at 0x7db76b5182e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f42710>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f42950>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b519780>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b51ab00>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b51b8b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b51a1d0>,\n",
              " <unstructured.documents.elements.Title at 0x7db76b51a6b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b51a2c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b51b2e0>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f427d0>,\n",
              " <unstructured.documents.elements.Table at 0x7db76b65de10>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b65c910>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b65dc90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b65e050>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b65f550>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a31f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a1c00>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a2380>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a0730>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a0130>,\n",
              " <unstructured.documents.elements.Title at 0x7db7690a0ac0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a0550>,\n",
              " <unstructured.documents.elements.Footer at 0x7db7690a1150>,\n",
              " <unstructured.documents.elements.Table at 0x7db7690a0df0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f426b0>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f41d80>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f40130>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f415d0>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f414e0>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f42830>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f41ae0>,\n",
              " <unstructured.documents.elements.Table at 0x7db7690a1480>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a1e70>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a1c60>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a29e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f42770>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a2a70>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a2bc0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a3d00>,\n",
              " <unstructured.documents.elements.Title at 0x7db7690a15a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a33d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a3190>,\n",
              " <unstructured.documents.elements.Footer at 0x7db7690a2bf0>,\n",
              " <unstructured.documents.elements.Table at 0x7db7690a3520>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f42f50>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f43790>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7db7690a1ed0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a3b50>,\n",
              " <unstructured.documents.elements.Title at 0x7db7690a2e60>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a3b80>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a39d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a08e0>,\n",
              " <unstructured.documents.elements.Title at 0x7db7690a0940>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a02b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a0610>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db7690a25c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db7690a1990>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db7690a03a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db7690a04c0>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f43bb0>,\n",
              " <unstructured.documents.elements.Title at 0x7db7690a1fc0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db7690a37f0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db7690a37c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f43e50>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f42560>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f43310>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f43d00>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f435e0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f41f30>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f425c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f43730>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f42f20>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f41180>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f41360>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f41210>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768f42aa0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c537f0>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f43c10>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c510c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c506a0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c52b60>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c528c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c532e0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c506d0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c52dd0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c53c10>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c53940>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c53010>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c53be0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c53df0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c501c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c53100>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c53e80>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c53400>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c52830>,\n",
              " <unstructured.documents.elements.Footer at 0x7db768c51030>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c52e60>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c533a0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c531c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c50700>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c51540>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c504f0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db768c51120>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41f430>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41e890>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41e4d0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41ddb0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41ffd0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41c9d0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41c970>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41ef80>,\n",
              " <unstructured.documents.elements.Footer at 0x7db76b41c940>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41eda0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41e8c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41f010>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41fb20>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b41fdf0>,\n",
              " <unstructured.documents.elements.Title at 0x7db76b41c880>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b41f3a0>,\n",
              " <unstructured.documents.elements.Title at 0x7db76b41e4a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b41cd00>,\n",
              " <unstructured.documents.elements.Header at 0x7db76b41c4f0>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f42170>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f43a60>,\n",
              " <unstructured.documents.elements.Table at 0x7db76b41dab0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f43a30>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b41e9e0>,\n",
              " <unstructured.documents.elements.Table at 0x7db76b41ec50>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f437f0>,\n",
              " <unstructured.documents.elements.Title at 0x7db768f43880>,\n",
              " <unstructured.documents.elements.Text at 0x7db768f43850>]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Header=[]\n",
        "Footer=[]\n",
        "Title=[]\n",
        "NarrativeText=[]\n",
        "Text=[]\n",
        "ListItem=[]\n",
        "\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
        "            Header.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
        "            Footer.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
        "            Title.append(str(element))\n",
        "  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "            NarrativeText.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
        "            Text.append(str(element))\n",
        "  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
        "            ListItem.append(str(element))"
      ],
      "metadata": {
        "id": "CIwL8HU-fmS3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Header"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbJx6ppgf5MV",
        "outputId": "f719f95f-a60f-4d2d-f279-ec4f3b1b1939"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t c O 5 1 ] L C . s c [', '13']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Footer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWPW4XN2gAX6",
        "outputId": "8b256b8c-ed62-4552-c120-6a880c56ece8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2', '3', '7', '8', '11', '12']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Title"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kawanaLtgCsu",
        "outputId": "46e20e9a-b346-4727-d618-e29ce47db606"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Uniﬁed Language Model Pre-training for Natural Language Understanding and Generation',\n",
              " 'Abstract',\n",
              " 'Introduction',\n",
              " '∗ Equal contribution. † Contact person.',\n",
              " 'Table 1: Comparison between language model (LM) pre-training objectives.',\n",
              " 'Backbone Network',\n",
              " 'Transformer with shared parameters for all LM objectives',\n",
              " '2 Uniﬁed Language Model Pre-training',\n",
              " 'Input Representation',\n",
              " '2.2 Backbone Network: Multi-Layer Transformer',\n",
              " '2.3 Pre-training Objectives',\n",
              " '2.4 Pre-training Setup',\n",
              " '2.5 Fine-tuning on Downstream NLU and NLG Tasks',\n",
              " '3 Experiments',\n",
              " '3.1 Abstractive Summarization',\n",
              " '2Wikipedia version: enwiki-20181101.',\n",
              " '3.2 Question Answering (QA)',\n",
              " '3.3 Question Generation',\n",
              " 'EM',\n",
              " 'F1',\n",
              " 'UNILM QA Model (Section 3.2) + UNILM Generated Questions',\n",
              " '3.4 Response Generation',\n",
              " 'Model',\n",
              " 'GPT 45.4 BERTLARGE 60.5 61.1 UNILM',\n",
              " '3.5 GLUE Benchmark',\n",
              " '4 Conclusion and Future Work',\n",
              " 'References',\n",
              " 'Appendix A Long Text Generation: A Case Study',\n",
              " 'Appendix B GLUE Benchmark',\n",
              " 'Input',\n",
              " 'Output',\n",
              " '7k/1.5k/1.4k Table 13: Summary of the GLUE benchmark.',\n",
              " 'Spearman corr']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NarrativeText"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqKFqquJgFrh",
        "outputId": "01952b28-17ca-41fb-bbf4-31f4a897a928"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This paper presents a new UNIﬁed pre-trained Language Model (UNILM) that can be ﬁne-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirec- tional, bidirectional, and sequence-to-sequence prediction. The uniﬁed modeling is achieved by employing a shared Transformer network and utilizing speciﬁc self-attention masks to control what context the prediction conditions on. UNILM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UNILM achieves new state-of- the-art results on ﬁve natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.',\n",
              " 'Language model (LM) pre-training has substantially advanced the state of the art across a variety of natural language processing tasks [8, 29, 19, 31, 9, 1]. Pre-trained LMs learn contextualized text representations by predicting words based on their context using large amounts of text data, and can be ﬁne-tuned to adapt to downstream tasks.',\n",
              " 'Different prediction tasks and training objectives have been used for pre-training LMs of different types, as shown in Table 1. ELMo [29] learns two unidirectional LMs: a forward LM reads the text from left to right, and a backward LM encodes the text from right to left. GPT [31] uses a left-to-right Transformer [43] to predict a text sequence word-by-word. In contrast, BERT [9] employs a bidirectional Transformer encoder to fuse both the left and right context to predict the masked words. Although BERT signiﬁcantly improves the performance of a wide range of natural language understanding tasks [9], its bidirectionality nature makes it difﬁcult to be applied to natural language generation tasks [44].',\n",
              " 'In this work we propose a new UNIﬁed pre-trained Language Model (UNILM) that can be applied to both natural language understanding (NLU) and natural language generation (NLG) tasks. UNILM is a multi-layer Transformer network, jointly pre-trained on large amounts of text, optimized for three types of unsupervised language modeling objectives as shown in Table 2. In particular, we design a',\n",
              " '33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.',\n",
              " 'Table 2: The uniﬁed LM is jointly pre-trained by multiple language modeling objectives, sharing the same parameters. We ﬁne-tune and evaluate the pre-trained uniﬁed LM on various datasets, including both language understanding and generation tasks.',\n",
              " 'set of cloze tasks [42] where a masked word is predicted based on its context. These cloze tasks differ in how the context is deﬁned. For a left-to-right unidirectional LM, the context of the masked word to be predicted consists of all the words on its left. For a right-to-left unidirectional LM, the context consists of all the words on the right. For a bidirectional LM, the context consists of the words on both the right and the left [9]. For a sequence-to-sequence LM, the context of the to-be-predicted word in the second (target) sequence consists of all the words in the ﬁrst (source) sequence and the words on the its left in the target sequence.',\n",
              " 'Similar to BERT, the pre-trained UNILM can be ﬁne-tuned (with additional task-speciﬁc layers if necessary) to adapt to various downstream tasks. But unlike BERT which is used mainly for NLU tasks, UNILM can be conﬁgured, using different self-attention masks (Section 2), to aggregate context for different types of language models, and thus can be used for both NLU and NLG tasks.',\n",
              " 'The proposed UNILM has three main advantages. First, the uniﬁed pre-training procedure leads to a single Transformer LM that uses the shared parameters and architecture for different types of LMs, alleviating the need of separately training and hosting multiple LMs. Second, the parameter sharing makes the learned text representations more general because they are jointly optimized for different language modeling objectives where context is utilized in different ways, mitigating overﬁtting to any single LM task. Third, in addition to its application to NLU tasks, the use of UNILM as a sequence-to-sequence LM (Section 2.3), makes it a natural choice for NLG, such as abstractive summarization and question generation.',\n",
              " 'Experimental results show that our model, used as a bidirectional encoder, compares favorably with BERT on the GLUE benchmark and two extractive question answering tasks (i.e., SQuAD 2.0 and CoQA). In addition, we demonstrate the effectiveness of UNILM on ﬁve NLG datasets, where it is used as a sequence-to-sequence model, creating new state-of-the-art results on CNN/DailyMail and Gigaword abstractive summarization, SQuAD question generation, CoQA generative question answering, and DSTC7 dialog response generation.',\n",
              " 'Given an input sequence x = x1 · · · x|x|, UNILM obtains a contextualized vector representation for each token. As shown in Figure 1, the pre-training optimizes the shared Transformer [43] network with respect to several unsupervised language modeling objectives, namely, unidirectional LM, bidirectional LM, and sequence-to-sequence LM. In order to control the access to the context of the word token to be predicted, we employ different masks for self-attention. In other words, we use masking to control how much context the token should attend to when computing its contextualized',\n",
              " 'Figure 1: Overview of uniﬁed LM pre-training. The model parameters are shared across the LM objectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM). We use different self-attention masks to control the access to context for each word token. The right-to-left LM is similar to the left-to-right one, which is omitted in the ﬁgure for brevity.',\n",
              " 'representation. Once UNILM is pretrained, we can ﬁne-tune it using task-speciﬁc data for downstream tasks.',\n",
              " 'The input x is a word sequence, which is either a text segment for unidirectional LMs or a pair of segments packed together for bidirectional LM and sequence-to-sequence LM. We always add a special start-of-sequence ([SOS]) token at the beginning of input, and a special end-of-sequence ([EOS]) token at the end of each segment. [EOS] not only marks the sentence boundary in NLU tasks, but also is used for the model to learn when to terminate the decoding process in NLG tasks. The input representation follows that of BERT [9]. Texts are tokenized to subword units by WordPiece [48]. For each input token, its vector representation is computed by summing the corresponding token embedding, position embedding, and segment embedding. Since UNILM is trained using multiple LM tasks, segment embeddings also play a role of LM identiﬁer in that we use different segment embeddings for different LM objectives.',\n",
              " 'The input vectors {xi}|x| i=1 is ﬁrst packed into H0 = [x1, · · · , x|x|], and then encoded into contextual representations at different levels of abstract Hl = [hl 1, · · · , hl |x|] using an L-layer Transformer Hl = Transformerl(Hl−1), l ∈ [1, L]. In each Transformer block, multiple self-attention heads are used to aggregate the output vectors of the previous layer. For the l-th Transformer layer, the output',\n",
              " 'of a self-attention head Al is computed via:',\n",
              " 'l , K = Hl−1WK allow to attend −∞, prevent from attending',\n",
              " 'where the previous layer’s output Hl−1 ∈ R|x|×dh is linearly projected to a triple of queries, keys and values using parameter matrices WQ l ∈ Rdh×dk , respectively, and the mask matrix l , WV M ∈ R|x|×|x| determines whether a pair of tokens can be attended to each other.',\n",
              " 'We use different mask matrices M to control what context a token can attend to when computing its contextualized representation, as illustrated in Figure 1. Take bidirectional LM as an example. The elements of the mask matrix are all 0s, indicating that all the tokens have access to each other.',\n",
              " 'We pretrain UNILM using four cloze tasks designed for different language modeling objectives. In a cloze task, we randomly choose some WordPiece tokens in the input, and replace them with special token [MASK]. Then, we feed their corresponding output vectors computed by the Transformer network into a softmax classiﬁer to predict the masked token. The parameters of UNILM are learned to minimize the cross-entropy loss computed using the predicted tokens and the original tokens. It is worth noting that the use of cloze tasks makes it possible to use the same training procedure for all LMs, unidirectional and bidirectional alike.',\n",
              " 'Unidirectional LM We use both left-to-right and right-to-left LM objectives. Take the left-to-right LM as an example. The representation of each token encodes only the leftward context tokens and itself. For instance, to predict the masked token of “x1x2 [MASK] x4”, only tokens x1, x2 and itself can be used. This is done by using a triangular matrix for the self-attention mask M (as in Equation (2)), where the upper triangular part of the self-attention mask is set to −∞, and the other elements to 0, as shown in Figure 1. Similarly, a right-to-left LM predicts a token conditioned on its future (right) context.',\n",
              " 'Bidirectional LM Following [9], a bidirectional LM allows all tokens to attend to each other in prediction. It encodes contextual information from both directions, and can generate better contextual representations of text than its unidirectional counterpart. As indicated in Equation (2), the self- attention mask M is a zero matrix, so that every token is allowed to attend across all positions in the input sequence.',\n",
              " 'Sequence-to-Sequence LM As shown in Figure 1, for prediction, the tokens in the ﬁrst (source) segment can attend to each other from both directions within the segment, while the tokens of the second (target) segment can only attend to the leftward context in the target segment and itself, as well as all the tokens in the source segment. For example, given source segment t1t2 and its target segment t3t4t5, we feed input “[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]” into the model. While both t1 and t2 have access to the ﬁrst four tokens, including [SOS] and [EOS], t4 can only attend to the ﬁrst six tokens.',\n",
              " 'Figure 1 shows the self-attention mask M used for the sequence-to-sequence LM objective. The left part of M is set to 0 so that all tokens can attend to the ﬁrst segment. The upper right part is set to −∞ to block attentions from the source segment to the target segment. Moreover, for the lower right part, we set its upper triangular part to −∞, and the other elements to 0, which prevents tokens in the target segment from attending their future (right) positions.',\n",
              " 'During training, we randomly choose tokens in both segments, and replace them with the special token [MASK]. The model is learned to recover the masked tokens. Since the pair of source and target texts are packed as a contiguous input text sequence in training, we implicitly encourage the model to learn the relationship between the two segments. In order to better predict tokens in the target segment, UNILM learns to effectively encode the source segment. Thus, the cloze task designed for',\n",
              " 'the sequence-to-sequence LM, also known as the encoder-decoder model, simultaneously pre-trains a bidirectional encoder and an unidirectional decoder. The pre-trained model, used as an encoder- decoder model, can be easily adapted to a wide range of conditional text generation tasks, such as abstractive summarization.',\n",
              " 'Next Sentence Prediction For the bidirectional LM, we also include the next sentence prediction task for pre-training, as in [9].',\n",
              " 'The overall training objective the sum of different types of LM objectives described above. Specif- ically, within one training batch, 1/3 of the time we use the bidirectional LM objective, 1/3 of the time we employ the sequence-to-sequence LM objective, and both left-to-right and right-to-left LM objectives are sampled with rate of 1/6. The model architecture of UNILM follows that of BERTLARGE [9] for a fair comparison. The gelu activation [18] is used as GPT [31]. Speciﬁcally, we use a 24-layer Transformer with 1, 024 hidden size, and 16 attention heads, which contains about 340M parameters. The weight matrix of the softmax classiﬁer is tied with token embeddings. UNILM is initialized by BERTLARGE, and then pre-trained using English Wikipedia2 and BookCorpus [53], which have been processed in the same way as [9]. The vocabulary size is 28, 996. The maximum length of input sequence is 512. The token masking probability is 15%. Among masked positions, 80% of the time we replace the token with [MASK], 10% of the time with a random token, and keeping the original token for the rest. In addition, 80% of the time we randomly mask one token each time, and 20% of the time we mask a bigram or a trigram.',\n",
              " 'Adam [22] with β1 = 0.9, β2 = 0.999 is used for optimization. The learning rate is 3e-5, with linear warmup over the ﬁrst 40, 000 steps and linear decay. The dropout rate is 0.1. The weight decay is 0.01. The batch size is 330. The pre-training procedure runs for about 770, 000 steps. It takes about 7 hours for 10, 000 steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training.',\n",
              " 'For NLU tasks, we ﬁne-tune UNILM as a bidirectional Transformer encoder, like BERT. Take text classiﬁcation as an example. We use the encoding vector of [SOS] as the representation of input, denoted as hL 1 , and feed it to a randomly initialized softmax classiﬁer (i.e., the task-speciﬁc output 1 WC), where WC ∈ Rdh×C is layer), where the class probabilities are computed as softmax(hL a parameter matrix, and C the number of categories. We maximize the likelihood of the labeled training data by updating the parameters of the pre-trained LM and the added softmax classiﬁer.',\n",
              " 'For NLG tasks, we take the sequence-to-sequence task as an example. The ﬁne-tuning procedure is similar to pre-training using the self-attention masks as in Section 2.3. Let S1 and S2 denote source and target sequences, respectively. We pack them together with special tokens, to form the input “[SOS] S1 [EOS] S2 [EOS]”. The model is ﬁne-tuned by masking some percentage of tokens in the target sequence at random, and learning to recover the masked words. The training objective is to maximize the likelihood of masked tokens given context. It is worth noting that [EOS], which marks the end of the target sequence, can also be masked during ﬁne-tuning, thus when this happens, the model learns when to emit [EOS] to terminate the generation process of the target sequence.',\n",
              " 'We have conducted experiments on both NLU (i.e., the GLUE benchmark, and extractive question answering) and NLG tasks (i.e., abstractive summarization, question generation, generative question answering, and dialog response generation).',\n",
              " 'Automatic text summarization produces a concise and ﬂuent summary conveying the key information in the input (e.g., a news article). We focus on abstractive summarization, a generation task where',\n",
              " 'Table 3: Evaluation results on CNN/DailyMail summarization. Models in the ﬁrst block are ex- tractive systems listed here for reference, while the others are abstractive models. The results of the best reported extractive model are taken from [27]. RG is short for ROUGE.',\n",
              " 'Table 4: Results on Gigaword abstractive summa- rization. Models in the ﬁrst block only use 10K examples for training, while the others use 3.8M examples. Results of OpenNMT and Transformer are taken from [4, 39]. RG is short for ROUGE.',\n",
              " 'the summary is not constrained to reusing the phrases or sentences in the input text. We use the non-anonymized version of the CNN/DailyMail dataset [37] and Gigaword [36] for model ﬁne-tuning and evaluation. We ﬁne-tune UNILM as a sequence-to-sequence model following the procedure described in Section 2.5 by concatenating document (the ﬁrst segment) and summary (the second segment) as input which is truncated according to a pre-deﬁned maximum length.',\n",
              " 'We ﬁne-tune our model on the training set for 30 epochs. We reuse most hyper-parameters from pre-training. The masking probability is 0.7. We also use label smoothing [40] with rate of 0.1. For CNN/DailyMail, we set batch size to 32, and maximum length to 768. For Gigaword, we set batch size to 64, and maximum length to 256. During decoding, we use beam search with beam size of 5. The input document is truncated to the ﬁrst 640 and 192 tokens for CNN/DailyMail and Gigaword, respectively. We remove duplicated trigrams in beam search, and tweak the maximum summary length on the development set [28, 13].',\n",
              " 'We use the F1 version of ROUGE [25] as the evaluation metric for both datasets. In Table 3, we compare UNILM against the baseline and several state-of-the-art models on CNN/DailyMail. LEAD- 3 is a baseline model that extracts the ﬁrst three sentences in a document as its summary. PGNet [37] is a sequence-to-sequence model based on the pointer-generator network. S2S-ELMo [13] uses a sequence-to-sequence model augmented with pre-trained ELMo representations, which is termed as SRC-ELMO+SHDEMB in [13]. Bottom-Up [16] is a sequence-to-sequence model augmented with a bottom-up content selector for selecting salient phrases. We also include in Table 3 the best reported extractive summarization result [27] on the dataset. As shown in Table 3, our model outperforms all previous abstractive systems, creating a new state-of-the-art abstractive summarization result on the dataset. Our model also outperforms the best extractive model [27] by 0.88 point in ROUGE-L.',\n",
              " 'In Table 4, we evaluate the models on Gigaword with different scales (10K and 3.8M). Both Transformer [43] and OpenNMT [23] implement standard attentional sequence-to-sequence models. Re3Sum [4] retrieves summaries as candidate templates, and then use an extended sequence-to- sequence model to generate summaries. MASS [39] is a pre-trained sequence-to-sequence model based on Transformer networks. Experimental results show that UNILM achieves better performance than previous work. Besides, in the low-resource setting (i.e., only 10,000 examples are used as training data), our model outperforms MASS by 7.08 point in ROUGE-L.',\n",
              " 'The task is to answer a question given a passage [33, 34, 15]. There are two settings. The ﬁrst is called extractive QA, where the answer is assumed to be a text span in the passage. The other is called generative QA, where the answer needs to be generated on the ﬂy.',\n",
              " 'Extractive QA This task can be formulated as a NLU task where we need to predict the start and end positions of the answer spans within the passage. We ﬁne-tune the pre-trained UNILM as a',\n",
              " 'Table 5: Extractive QA results on the SQuAD development set. Table 6: Extractive QA results on the CoQA development set. Table 7: Generative QA results on the CoQA development set.',\n",
              " 'bidirectional encoder for the task. We conduct experiments on the Stanford Question Answering Dataset (SQuAD) 2.0 [34], and Conversational Question Answering (CoQA) [35] datasets.',\n",
              " 'The results on SQuAD 2.0 are reported in Table 5, where we compare two models in Exact Match (EM) and F1 score. RMR+ELMo [20] is an LSTM-based question answering model augmented with pre-trained language representation. BERTLARGE is a cased model, ﬁne-tuned on the SQuAD training data for 3 epochs, with batch size 24, and maximum length 384. UNILM is ﬁne-tuned in the same way as BERTLARGE. We see that UNILM outperforms BERTLARGE.',\n",
              " 'CoQA is a conversational question answering dataset. Compared with SQuAD, CoQA has several unique characteristics. First, the examples in CoQA are conversational, so we need to answer the input question based on conversation histories. Second, the answers in CoQA can be free-form texts, including a large portion is of yes/no answers.',\n",
              " 'We modify the model used for SQuAD as follows. Firstly, in addition to the asked question, we concatenate the question-answer histories to the ﬁrst segment, so that the model can capture conversational information. Secondly, for yes/no questions, we use the ﬁnal hidden vector of the [SOS] token to predict whether the input is a yes/no question, and whether the answer is yes or no. For other examples, we select a passage subspan with the highest F1 score for training.',\n",
              " 'The results on CoQA are reported in Table 6, where we compare two models in F1 scores. DrQA+ELMo [35] is an LSTM-based question answering model augmented with pre-trained ELMo representation. BERTLARGE is a cased model, ﬁne-tuned on the CoQA training data for 2 epochs, with batch size 16, and maximum length 512. UNILM is ﬁne-tuned with the same hyper-parameters as BERTLARGE. We see that UNILM outperforms BERTLARGE.',\n",
              " 'Generative QA Generative question answering generates free-form answers for the input question and passage, which is a NLG task. In contrast, extractive methods can only predict subspans of the input passage as answers. On the CoQA dataset (as described above), Reddy et al. [2019] show that vanilla sequence-to-sequence models still underperforms extractive methods by a wide margin.',\n",
              " 'We adapt UNILM to generative question answering as a sequence-to-sequence model. The ﬁrst segment (i.e., the input sequence) is the concatenation of conversational histories, the input question and the passage. The second segment (i.e., the output sequence) is the answer. We ﬁne-tune the pre-trained UNILM on the CoQA training set for 10 epochs. We set the batch size to 32, the mask probability to 0.5, and the maximum length to 512. We also use label smoothing with rate of 0.1. The other hyper-parameters are kept the same as pre-training. During decoding, we use beam search with beam size of 3. The maximum length of input question and passage is 470. For passages that are longer than the maximum length, we split the passage into several chunks with a sliding window approach, and select a chunk with the highest word overlap over the question.',\n",
              " 'We compare our method with the generative question answering models Seq2Seq and PGNet as described in [35]. The Seq2Seq baseline is a sequence-to-sequence model with an attention mech- anism. The PGNet model augments Seq2Seq with a copy mechanism. As shown in Table 7, our generative question answering model outperforms previous generative methods by a wide margin, which signiﬁcantly closes the gap between generative method and extractive method.',\n",
              " 'We conduct experiments for the answer-aware question generation task [52]. Given an input passage and an answer span, our goal is to generate a question that asks for the answer. The SQuAD 1.1 dataset [33] is used for evaluation. Following [12], we split the original training set into training and',\n",
              " '16.38 20.76 23.75 Table 8: Question generation results on SQuAD. MTR is short for METEOR, and RG for ROUGE. Results in the groups use different data splits.',\n",
              " 'Table 9: Question generation based on UNILM improves question answering results on the SQuAD development set.',\n",
              " 'Table 10: Response generation results. Div-1 and Div-2 indicate diversity of unigrams and bigrams, respectively.',\n",
              " 'test sets, and keep the original development set. We also conduct experiments following the data split as in [51], which uses the reversed dev-test split.',\n",
              " 'The question generation task is formulated as a sequence-to-sequence problem. The ﬁrst segment is the concatenation of input passage and answer, while the second segment is the generated question.',\n",
              " 'We ﬁne-tune UNILM on the training set for 10 epochs. We set batch size to 32, masking probability to 0.7, and learning rate to 2e-5. The rate of label smoothing is 0.1. The other hyper-parameters are the same as pre-training. During decoding, we truncate the input to 464 tokens by selecting a passage chunk which contains the answer. The evaluation metrics BLEU-4, METEOR, and ROUGE-L are computed by the same scripts as in [12]. The results3 are presented in Table 8. CorefNQG [11] is based on a sequence-to-sequence model with attention and a feature-rich encoder. MP-GSN [51] uses an attention-based sequence-to-sequence model with a gated self-attention encoder. SemQG [50] uses two semantics-enhanced rewards to regularize the generation. UNILM outperforms previous models and achieves a new state-of-the-art for question generation.',\n",
              " 'Generated Questions Improve QA The question generation model can automatically harvest a large number of question-passage-answer examples from a text corpus. We show that the augmented data generated by question generation improves the question answering model.',\n",
              " 'We generate ﬁve million answerable examples, and four million unanswerable examples by modifying the answerable ones. We ﬁne-tune our question answering model on the generated data for one epoch. Then the model is ﬁne-tuned on the SQuAD 2.0 data for two more epochs.',\n",
              " 'As shown in Table 9, the augmented data generated by UNILM improves question answering model introduced in Section 3.2. Note that we use bidirectional masked language modeling as an auxiliary task for both the generated and SQuAD 2.0 datasets during ﬁne-tuning, which brings 2.3 absolute improvement compared to directly using automatically generated examples. A possible reason is that the auxiliary task alleviates catastrophic forgetting [49] when ﬁne-tuning on augmented data.',\n",
              " 'We evaluate UNILM on the document-grounded dialog response generation task [30, 15]. Given a multi-turn conversation history and a web document as the knowledge source, the system needs to',\n",
              " '3Notice that if we directly use the tokenized references provided by Du et al. [2017], the results are (21.63 BLEU-4 / 25.04 METEOR / 51.09 ROUGE-L) on the original data split [12], and (23.08 BLEU-4 / 25.57 METEOR / 52.03 ROUGE-L) in the reversed dev-test setup [51].',\n",
              " 'generate a natural language response that is both conversationally appropriate and reﬂective of the contents of the web document. We ﬁne-tune UNILM to the task as a sequence-to-sequence model. The ﬁrst segment (input sequence) is the concatenation of the web document and the conversation history. The second segment (output sequence) is the response. We ﬁne-tune UNILM on the DSTC7 training data for 20 epochs, with batch size 64. The masking probability is set to 0.5. The maximum length is 512. During decoding, we use beam search with size of 10. The maximum length of generated response is set to 40. As shown in Table 10, UNILM outperforms the best system [41] in the DSTC7 shared task [14] across all evaluation metrics.',\n",
              " 'We evaluate UNILM on the General Language Understanding Evaluation (GLUE) benchmark [45]. GLUE is a collection of nine language understanding tasks, including question answering [33], linguistic acceptability [46], sentiment analysis [38], text similarity [5], paraphrase detection [10], and natural language inference (NLI) [7, 2, 17, 3, 24, 47].',\n",
              " 'Our model is ﬁne-tuned as a bidirectional LM. We use Adamax [21] as our optimizer with a learning rate of 5e-5 and a batch size of 32. The maximum number of epochs is set to 5. A linear learning rate decay schedule with warmup of 0.1 is used. The dropout rate of the last linear projection for each task is set to 0.1, except 0.3 for MNLI and 0.05 for CoLA/SST-2. To avoid the gradient explosion issue, the gradient norm was clipped within 1. We truncated the tokens no longer than 512.',\n",
              " 'Table 11 presents the GLUE test results obtained from the benchmark evaluation server. The results show that UNILM obtains comparable performance on the GLUE tasks in comparison with BERTLARGE.',\n",
              " 'We propose a uniﬁed pre-training model, UNILM, which is jointly optimized for several LM objectives with shared parameters. The uniﬁcation of bidirectional, unidirectional, and sequence- to-sequence LMs enables us to straightforwardly ﬁne-tune the pre-trained UNILM for both NLU and NLG tasks. Experimental results demonstrate that our model compares favorably with BERT on the GLUE benchmark and two question answering datasets. In addition, UNILM outperforms previous state-of-the-art models on ﬁve NLG datasets: CNN/DailyMail and Gigaword abstractive summarization, SQuAD question generation, CoQA generative question answering, and DSTC7 dialog response generation.',\n",
              " 'The work can be advanced from the following perspectives:',\n",
              " 'Acknowledgement We would like to acknowledge Shiyue Zhang for the helpful discussions about the question generation experiments.',\n",
              " 'Our model can generate text samples using the left-to-right setting. We picked three text samples sampled from left to right using our model, as shown in Table 12. We use the top-40 truncating sampling strategy [32], and forbid duplicate 4-grams during generation. For each example, we sampled 10 times from the same input and we hand-picked the best one; as such, these samples should be considered to be better than the average model output. From the examples, we ﬁnd that the model can produce ﬂuent output with somewhat consistent contents which ﬁts the inputs’ genres and topics. In the ﬁrst example, given a modiﬁed excerpt from the novel “1984” as input, the model’s output is like a short paragraph in an fantasy novel; for the other two with input from Wikipedia, the model makes up “facts” with plausible names, time stamps and events written in the Wikipedia style.',\n",
              " 'As shown in Table 13, we summarize the data size and the evaluation metrics used for the General Language Understanding Evaluation (GLUE) benchmark.',\n",
              " 'Maurice Ravel (7 March 1875 - 28 December 1937) was a French composer, pianist and conductor. He was born in Paris, and studied piano with Eugene Massenet. In the early 1900s Ravel became interested in the composition of music, and he composed much of his early music for the piano. Ravel’s early work is marked by a marked sense of lyricism, but in the later years of his career he developed a strong preference for orchestral forms. His ﬁrst work, “le Grand Cours d’ Auvergne”, was composed in 1909 by Maurice Ravel, a student of Jules Massenet, and was published by A.F.A. in 1912. It was re-published in 1912 by the publisher J.S.D.M. de l’Etablissement Musicale de la Musique Francaise. Ravel wrote the piano concerto “la Tragedie et la Chanson Dans le Theatre des Champs Elysees” in 1916. (...)',\n",
              " 'Table 12: Text samples generated by our model using left-to-right generation.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CdG3HMEgMsB",
        "outputId": "3d1d7d6b-cc31-4191-c666-d4714da7cf07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9 1 0 2',\n",
              " '3 v 7 9 1 3 0 . 5 0 9 1 : v i X r a',\n",
              " 'Li Dong∗ Nan Yang∗ Wenhui Wang∗ Furu Wei∗ † Xiaodong Liu Yu Wang Jianfeng Gao Ming Zhou Hsiao-Wuen Hon Microsoft Research {lidong1,nanya,wenwan,fuwei}@microsoft.com {xiaodl,yuwan,jfgao,mingzhou,hon}@microsoft.com',\n",
              " \"Q=H'w?, 0,\",\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '80.5 84.7',\n",
              " '83.4 87.6',\n",
              " '9',\n",
              " '10',\n",
              " '14']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ListItem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZy-_Z5UgN9O",
        "outputId": "860ef7e2-058e-43cc-aa5c-53aeb7e71943"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['• We will push the limit of the current method by training more epochs and larger models on web- scale text corpora. At the same time, we will also conduct more experiments on end applications as well as ablation experiments to investigate the model capability and the beneﬁts of pre-training multiple language modeling tasks with the same network.',\n",
              " '• We are focusing on monolingual NLP tasks in our current experiments. We are also interested in extending UNILM to support cross-lingual tasks [6].',\n",
              " '• We will conduct multi-task ﬁne-tuning on both NLU and NLG tasks, which is a natural extension of Multi-Task Deep Neural Network (MT-DNN) [26].',\n",
              " '[1] Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven pretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019.',\n",
              " '[2] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second PASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 01 2006.',\n",
              " '[3] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The ﬁfth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC-09), 2009.',\n",
              " '[4] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template based neural summarization. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 152–161, Melbourne, Australia, July 2018. Association for Computational Linguistics.',\n",
              " '[5] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.',\n",
              " '[6] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang. Cross- lingual natural language generation via pre-training. ArXiv, abs/1909.10481, 2019.',\n",
              " '[7] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entail- ment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classiﬁcation, and Recognizing Textual Entailment, MLCW’05, pages 177–190, Berlin, Heidelberg, 2006. Springer-Verlag.',\n",
              " '[8] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems 28, pages 3079–3087. Curran Associates, Inc., 2015.',\n",
              " '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.',\n",
              " '[10] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential para- phrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.',\n",
              " '[11] Xinya Du and Claire Cardie. Harvesting paragraph-level question-answer pairs from Wikipedia. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 1907–1917, Melbourne, Australia, July 2018. Association for Computational Linguistics.',\n",
              " '[12] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1342–1352, 2017.',\n",
              " '[13] Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations for language generation. CoRR, abs/1903.09722, 2019.',\n",
              " '[14] Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. Grounded response generation task at dstc7. In AAAI Dialog System Technology Challenges Workshop, 2019.',\n",
              " '[15] Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Founda- tions and Trends in Information Retrieval, 13(2-3):127–298, 2019.',\n",
              " '[16] Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098–4109, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.',\n",
              " '[17] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL In Proceedings of the ACL-PASCAL Workshop recognizing textual entailment challenge. on Textual Entailment and Paraphrasing, pages 1–9, Prague, June 2007. Association for Computational Linguistics.',\n",
              " '[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016.',\n",
              " '[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi- ﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational Linguistics.',\n",
              " '[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify: Machine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018.',\n",
              " '[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.',\n",
              " '[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, San Diego, CA, 2015.',\n",
              " '[23] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT: In Proceedings of ACL 2017, System Open-source toolkit for neural machine translation. Demonstrations, pages 67–72, Vancouver, Canada, July 2017. Association for Computational Linguistics.',\n",
              " '[24] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.',\n",
              " '[25] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza- tion Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.',\n",
              " '[26] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. CoRR, abs/1901.11504, 2019.',\n",
              " '[27] Yang Liu. Fine-tune BERT for extractive summarization. CoRR, abs/1903.10318, 2019.',\n",
              " '[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. CoRR, abs/1705.04304, 2018.',\n",
              " '[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.',\n",
              " '[30] Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin Choi, and Jianfeng Gao. Conversing by reading: Contentful neural conversation with on- In Proceedings of the 57th Annual Meeting of the Association demand machine reading. for Computational Linguistics, pages 5427–5436, Florence, Italy, July 2019. Association for Computational Linguistics.',\n",
              " '[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.',\n",
              " '[32] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.',\n",
              " '[33] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ ques- tions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics.',\n",
              " '[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable In Proceedings of the 56th Annual Meeting of the Association for questions for SQuAD. Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, pages 784–789, 2018.',\n",
              " '[35] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249–266, March 2019.',\n",
              " '[36] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal, September 2015. Association for Computational Linguistics.',\n",
              " '[37] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1073–1083, Vancouver, Canada, July 2017. Association for Computational Linguistics.',\n",
              " '[38] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642, 2013.',\n",
              " '[39] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.',\n",
              " '[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re- thinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818–2826, 2016.',\n",
              " '[41] Y Tam, Jiachen Ding, Cheng Niu, and Jie Zhou. Cluster-based beam search for pointer-generator chatbot grounded by knowledge. In AAAI Dialog System Technology Challenges Workshop, 2019.',\n",
              " '[42] Wilson L Taylor. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415–433, 1953.',\n",
              " '[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017.',\n",
              " '[44] Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov random ﬁeld language model. CoRR, abs/1902.04094, 2019.',\n",
              " '[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019.',\n",
              " '[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.',\n",
              " '[47] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.',\n",
              " '[48] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016.',\n",
              " '[49] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and arXiv preprint Phil Blunsom. Learning and evaluating general linguistic intelligence. arXiv:1901.11373, 2019.',\n",
              " '[50] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi- supervised question answering. CoRR, abs/1909.06356, 2019.',\n",
              " '[51] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question generation with maxout pointer and gated self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3901–3910, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.',\n",
              " '[52] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural question generation from text: A preliminary study. In Xuanjing Huang, Jing Jiang, Dongyan Zhao, Yansong Feng, and Yu Hong, editors, Natural Language Processing and Chinese Computing, pages 662–671. Springer International Publishing, 2018.',\n",
              " '[53] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE International Conference on Computer Vision, pages 19–27, 2015.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements2=partition_pdf(\n",
        "    filename=\"/content/data2/OPEN AI.pdf\",\n",
        "    strategy=\"hi_res\",\n",
        "    extract_images_in_pdf=True,\n",
        "    extract_image_block_types=[\"Image\", \"Table\"],\n",
        "    extract_image_block_to_payload=False,\n",
        "    extract_image_block_output_dir=\"extracted_data2\"\n",
        "  )\n",
        ""
      ],
      "metadata": {
        "id": "sIr4qgpygT_z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUVXZ4m5gpAb",
        "outputId": "db69cef9-defd-46e3-8470-da0b6caa5686"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.Title at 0x7db7690a21a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768eae6b0>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b41f8e0>,\n",
              " <unstructured.documents.elements.Title at 0x7db76b41e830>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b41f9d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b41d4b0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b45c8e0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b60c190>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b4872e0>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b485ed0>,\n",
              " <unstructured.documents.elements.Title at 0x7db76b4846a0>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b484a30>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b486230>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b4879d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45cfd0>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45c460>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45ec20>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45d690>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45c580>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45c820>,\n",
              " <unstructured.documents.elements.Title at 0x7db76b45d090>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45e8f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45f340>,\n",
              " <unstructured.documents.elements.Title at 0x7db76b45f4c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45d2d0>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45c8b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45cc70>,\n",
              " <unstructured.documents.elements.ListItem at 0x7db76b45d7e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45f520>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45ee90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45e470>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45efe0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45e2f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45e200>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45e140>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45e0e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45dd50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45dcf0>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45ecb0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45f580>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45d990>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45c0a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45cdf0>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b45f400>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db76b45d3f0>,\n",
              " <unstructured.documents.elements.Title at 0x7db76b45e8c0>,\n",
              " <unstructured.documents.elements.Image at 0x7db768f39ff0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f39b40>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768f39cc0>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b41e7a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7db768d82a70>,\n",
              " <unstructured.documents.elements.Title at 0x7db768d81990>,\n",
              " <unstructured.documents.elements.Image at 0x7db76b484a90>]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Header1=[]\n",
        "Footer1=[]\n",
        "Title1=[]\n",
        "NarrativeText1=[]\n",
        "Text1=[]\n",
        "ListItem1=[]\n",
        "\n",
        "\n",
        "for element in raw_pdf_elements2:\n",
        "  if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
        "            Header1.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
        "            Footer1.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
        "            Title1.append(str(element))\n",
        "  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "            NarrativeText1.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
        "            Text1.append(str(element))\n",
        "  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
        "            ListItem1.append(str(element))"
      ],
      "metadata": {
        "id": "yZuWpDcXgvg6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Header1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOjYXGPTgvoU",
        "outputId": "24d30790-7972-4d38-a997-56eacb4d5341"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Footer1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLWjZD7Ygvr0",
        "outputId": "45d292f2-e4d8-4eaf-cb6d-2fcb10fd1d0f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Title1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w0D7fCqg6c1",
        "outputId": "ff5d4740-5339-4485-b3a9-79a3978bc8c0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['OPEN AI',\n",
              " 'Click on the API',\n",
              " 'Environment created',\n",
              " 'Generate OpenAI API key:',\n",
              " 'https://platform.openai.com/playground',\n",
              " 'Pypi Open AI',\n",
              " 'https://openai.com/pricing']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NarrativeText1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojyIBeZNg6fg",
        "outputId": "74584d8e-5b77-4e93-ff80-e1fd7b56eeff"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Step 1: Login in to your account',\n",
              " 'Things to install when we want to use openapi’s',\n",
              " 'Open anaconda prompt and create a new environment',\n",
              " 'Now we need to activate the environment',\n",
              " 'Pip list',\n",
              " 'Install the Jupyter notebook',\n",
              " 'Open Jupyter notebook and install the open AI library',\n",
              " 'Once you generated the openAPI key, then open the OpenAI playground',\n",
              " 'in the assistants tab you will see the two options like Assistants and chat',\n",
              " 'There you will find three options',\n",
              " 'System – How your model is going to behave or behavior of the system. So here we need to set the behavior of the system.',\n",
              " 'Temperature: If you set the higher value of temperature which means it indicates that give me a more creative answer and adding randomness',\n",
              " 'If the value is zero then it indicates that gives the straight forward answer.',\n",
              " 'Maximum length – Token length',\n",
              " 'Top – Adding diversity inside our output',\n",
              " 'Frequency penalty – if we don’t want to repeat the tokens inside our output then we need to mention the frequency penalty.',\n",
              " 'Click view code',\n",
              " 'We will get the entire python code',\n",
              " 'Now we change the behavior the system.',\n",
              " 'See the differences in the output while we change the behavior of the system.',\n",
              " 'Assistants in open ai playground:',\n",
              " 'While discuss the assistant’s part in a detailed way while we implement in the project section.',\n",
              " 'Open AI tokens',\n",
              " 'https://platform.openai.com/tokenizer',\n",
              " 'Pricing']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Text1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8BboaZpg6i-",
        "outputId": "1396d2dd-4b6d-46f5-d503-98a58d4de570"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ListItem1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RSLNNAPhDmv",
        "outputId": "89bd7126-6ad4-409f-d9f1-ac7d5655871d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. Anaconda 2. Python 3. Jupyter notebook 4. Create virtual env and activate the env 5.',\n",
              " 'Install the dependencies or required packages (open ai, ……)',\n",
              " '1. System 2. User 3. Model']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img=[]\n",
        "for element in raw_pdf_elements2:\n",
        "  if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
        "            img.append(str(element))"
      ],
      "metadata": {
        "id": "QGpVpr-9hvuV"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image in encoded format\n",
        "img[0]\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wkOGf6_ziFFe",
        "outputId": "ee164b13-5616-40cc-8725-8471bab13300"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'G Openal ChatGPT > API > Interact wth our flagship Integrate Open models lactase fetrereetenee conversational interface'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tab=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "            tab.append(str(element))"
      ],
      "metadata": {
        "id": "Z1FuDFPUiG_9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "FRmnvih0jEye",
        "outputId": "66382a77-827e-44d3-f059-e630b98c1d6e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LM Objectives of Uniﬁed Pre-training What Uniﬁed LM Learns Example Downstream Tasks Bidirectional LM Unidirectional LM Sequence-to-Sequence LM Bidirectional encoding Unidirectional decoding Unidirectional decoding conditioned on bidirectional encoding GLUE benchmark Extractive question answering Long text generation Abstractive summarization Question generation Generative question answering'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NarrativeText=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "            NarrativeText.append(str(element))"
      ],
      "metadata": {
        "id": "onfhVgR8jF5D"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NarrativeText"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsV5BwicjlEV",
        "outputId": "9681cd7f-6d9a-4bdb-e6f0-fcce1a68c561"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This paper presents a new UNIﬁed pre-trained Language Model (UNILM) that can be ﬁne-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirec- tional, bidirectional, and sequence-to-sequence prediction. The uniﬁed modeling is achieved by employing a shared Transformer network and utilizing speciﬁc self-attention masks to control what context the prediction conditions on. UNILM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UNILM achieves new state-of- the-art results on ﬁve natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.',\n",
              " 'Language model (LM) pre-training has substantially advanced the state of the art across a variety of natural language processing tasks [8, 29, 19, 31, 9, 1]. Pre-trained LMs learn contextualized text representations by predicting words based on their context using large amounts of text data, and can be ﬁne-tuned to adapt to downstream tasks.',\n",
              " 'Different prediction tasks and training objectives have been used for pre-training LMs of different types, as shown in Table 1. ELMo [29] learns two unidirectional LMs: a forward LM reads the text from left to right, and a backward LM encodes the text from right to left. GPT [31] uses a left-to-right Transformer [43] to predict a text sequence word-by-word. In contrast, BERT [9] employs a bidirectional Transformer encoder to fuse both the left and right context to predict the masked words. Although BERT signiﬁcantly improves the performance of a wide range of natural language understanding tasks [9], its bidirectionality nature makes it difﬁcult to be applied to natural language generation tasks [44].',\n",
              " 'In this work we propose a new UNIﬁed pre-trained Language Model (UNILM) that can be applied to both natural language understanding (NLU) and natural language generation (NLG) tasks. UNILM is a multi-layer Transformer network, jointly pre-trained on large amounts of text, optimized for three types of unsupervised language modeling objectives as shown in Table 2. In particular, we design a',\n",
              " '33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.',\n",
              " 'Table 2: The uniﬁed LM is jointly pre-trained by multiple language modeling objectives, sharing the same parameters. We ﬁne-tune and evaluate the pre-trained uniﬁed LM on various datasets, including both language understanding and generation tasks.',\n",
              " 'set of cloze tasks [42] where a masked word is predicted based on its context. These cloze tasks differ in how the context is deﬁned. For a left-to-right unidirectional LM, the context of the masked word to be predicted consists of all the words on its left. For a right-to-left unidirectional LM, the context consists of all the words on the right. For a bidirectional LM, the context consists of the words on both the right and the left [9]. For a sequence-to-sequence LM, the context of the to-be-predicted word in the second (target) sequence consists of all the words in the ﬁrst (source) sequence and the words on the its left in the target sequence.',\n",
              " 'Similar to BERT, the pre-trained UNILM can be ﬁne-tuned (with additional task-speciﬁc layers if necessary) to adapt to various downstream tasks. But unlike BERT which is used mainly for NLU tasks, UNILM can be conﬁgured, using different self-attention masks (Section 2), to aggregate context for different types of language models, and thus can be used for both NLU and NLG tasks.',\n",
              " 'The proposed UNILM has three main advantages. First, the uniﬁed pre-training procedure leads to a single Transformer LM that uses the shared parameters and architecture for different types of LMs, alleviating the need of separately training and hosting multiple LMs. Second, the parameter sharing makes the learned text representations more general because they are jointly optimized for different language modeling objectives where context is utilized in different ways, mitigating overﬁtting to any single LM task. Third, in addition to its application to NLU tasks, the use of UNILM as a sequence-to-sequence LM (Section 2.3), makes it a natural choice for NLG, such as abstractive summarization and question generation.',\n",
              " 'Experimental results show that our model, used as a bidirectional encoder, compares favorably with BERT on the GLUE benchmark and two extractive question answering tasks (i.e., SQuAD 2.0 and CoQA). In addition, we demonstrate the effectiveness of UNILM on ﬁve NLG datasets, where it is used as a sequence-to-sequence model, creating new state-of-the-art results on CNN/DailyMail and Gigaword abstractive summarization, SQuAD question generation, CoQA generative question answering, and DSTC7 dialog response generation.',\n",
              " 'Given an input sequence x = x1 · · · x|x|, UNILM obtains a contextualized vector representation for each token. As shown in Figure 1, the pre-training optimizes the shared Transformer [43] network with respect to several unsupervised language modeling objectives, namely, unidirectional LM, bidirectional LM, and sequence-to-sequence LM. In order to control the access to the context of the word token to be predicted, we employ different masks for self-attention. In other words, we use masking to control how much context the token should attend to when computing its contextualized',\n",
              " 'Figure 1: Overview of uniﬁed LM pre-training. The model parameters are shared across the LM objectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM). We use different self-attention masks to control the access to context for each word token. The right-to-left LM is similar to the left-to-right one, which is omitted in the ﬁgure for brevity.',\n",
              " 'representation. Once UNILM is pretrained, we can ﬁne-tune it using task-speciﬁc data for downstream tasks.',\n",
              " 'The input x is a word sequence, which is either a text segment for unidirectional LMs or a pair of segments packed together for bidirectional LM and sequence-to-sequence LM. We always add a special start-of-sequence ([SOS]) token at the beginning of input, and a special end-of-sequence ([EOS]) token at the end of each segment. [EOS] not only marks the sentence boundary in NLU tasks, but also is used for the model to learn when to terminate the decoding process in NLG tasks. The input representation follows that of BERT [9]. Texts are tokenized to subword units by WordPiece [48]. For each input token, its vector representation is computed by summing the corresponding token embedding, position embedding, and segment embedding. Since UNILM is trained using multiple LM tasks, segment embeddings also play a role of LM identiﬁer in that we use different segment embeddings for different LM objectives.',\n",
              " 'The input vectors {xi}|x| i=1 is ﬁrst packed into H0 = [x1, · · · , x|x|], and then encoded into contextual representations at different levels of abstract Hl = [hl 1, · · · , hl |x|] using an L-layer Transformer Hl = Transformerl(Hl−1), l ∈ [1, L]. In each Transformer block, multiple self-attention heads are used to aggregate the output vectors of the previous layer. For the l-th Transformer layer, the output',\n",
              " 'of a self-attention head Al is computed via:',\n",
              " 'l , K = Hl−1WK allow to attend −∞, prevent from attending',\n",
              " 'where the previous layer’s output Hl−1 ∈ R|x|×dh is linearly projected to a triple of queries, keys and values using parameter matrices WQ l ∈ Rdh×dk , respectively, and the mask matrix l , WV M ∈ R|x|×|x| determines whether a pair of tokens can be attended to each other.',\n",
              " 'We use different mask matrices M to control what context a token can attend to when computing its contextualized representation, as illustrated in Figure 1. Take bidirectional LM as an example. The elements of the mask matrix are all 0s, indicating that all the tokens have access to each other.',\n",
              " 'We pretrain UNILM using four cloze tasks designed for different language modeling objectives. In a cloze task, we randomly choose some WordPiece tokens in the input, and replace them with special token [MASK]. Then, we feed their corresponding output vectors computed by the Transformer network into a softmax classiﬁer to predict the masked token. The parameters of UNILM are learned to minimize the cross-entropy loss computed using the predicted tokens and the original tokens. It is worth noting that the use of cloze tasks makes it possible to use the same training procedure for all LMs, unidirectional and bidirectional alike.',\n",
              " 'Unidirectional LM We use both left-to-right and right-to-left LM objectives. Take the left-to-right LM as an example. The representation of each token encodes only the leftward context tokens and itself. For instance, to predict the masked token of “x1x2 [MASK] x4”, only tokens x1, x2 and itself can be used. This is done by using a triangular matrix for the self-attention mask M (as in Equation (2)), where the upper triangular part of the self-attention mask is set to −∞, and the other elements to 0, as shown in Figure 1. Similarly, a right-to-left LM predicts a token conditioned on its future (right) context.',\n",
              " 'Bidirectional LM Following [9], a bidirectional LM allows all tokens to attend to each other in prediction. It encodes contextual information from both directions, and can generate better contextual representations of text than its unidirectional counterpart. As indicated in Equation (2), the self- attention mask M is a zero matrix, so that every token is allowed to attend across all positions in the input sequence.',\n",
              " 'Sequence-to-Sequence LM As shown in Figure 1, for prediction, the tokens in the ﬁrst (source) segment can attend to each other from both directions within the segment, while the tokens of the second (target) segment can only attend to the leftward context in the target segment and itself, as well as all the tokens in the source segment. For example, given source segment t1t2 and its target segment t3t4t5, we feed input “[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]” into the model. While both t1 and t2 have access to the ﬁrst four tokens, including [SOS] and [EOS], t4 can only attend to the ﬁrst six tokens.',\n",
              " 'Figure 1 shows the self-attention mask M used for the sequence-to-sequence LM objective. The left part of M is set to 0 so that all tokens can attend to the ﬁrst segment. The upper right part is set to −∞ to block attentions from the source segment to the target segment. Moreover, for the lower right part, we set its upper triangular part to −∞, and the other elements to 0, which prevents tokens in the target segment from attending their future (right) positions.',\n",
              " 'During training, we randomly choose tokens in both segments, and replace them with the special token [MASK]. The model is learned to recover the masked tokens. Since the pair of source and target texts are packed as a contiguous input text sequence in training, we implicitly encourage the model to learn the relationship between the two segments. In order to better predict tokens in the target segment, UNILM learns to effectively encode the source segment. Thus, the cloze task designed for',\n",
              " 'the sequence-to-sequence LM, also known as the encoder-decoder model, simultaneously pre-trains a bidirectional encoder and an unidirectional decoder. The pre-trained model, used as an encoder- decoder model, can be easily adapted to a wide range of conditional text generation tasks, such as abstractive summarization.',\n",
              " 'Next Sentence Prediction For the bidirectional LM, we also include the next sentence prediction task for pre-training, as in [9].',\n",
              " 'The overall training objective the sum of different types of LM objectives described above. Specif- ically, within one training batch, 1/3 of the time we use the bidirectional LM objective, 1/3 of the time we employ the sequence-to-sequence LM objective, and both left-to-right and right-to-left LM objectives are sampled with rate of 1/6. The model architecture of UNILM follows that of BERTLARGE [9] for a fair comparison. The gelu activation [18] is used as GPT [31]. Speciﬁcally, we use a 24-layer Transformer with 1, 024 hidden size, and 16 attention heads, which contains about 340M parameters. The weight matrix of the softmax classiﬁer is tied with token embeddings. UNILM is initialized by BERTLARGE, and then pre-trained using English Wikipedia2 and BookCorpus [53], which have been processed in the same way as [9]. The vocabulary size is 28, 996. The maximum length of input sequence is 512. The token masking probability is 15%. Among masked positions, 80% of the time we replace the token with [MASK], 10% of the time with a random token, and keeping the original token for the rest. In addition, 80% of the time we randomly mask one token each time, and 20% of the time we mask a bigram or a trigram.',\n",
              " 'Adam [22] with β1 = 0.9, β2 = 0.999 is used for optimization. The learning rate is 3e-5, with linear warmup over the ﬁrst 40, 000 steps and linear decay. The dropout rate is 0.1. The weight decay is 0.01. The batch size is 330. The pre-training procedure runs for about 770, 000 steps. It takes about 7 hours for 10, 000 steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training.',\n",
              " 'For NLU tasks, we ﬁne-tune UNILM as a bidirectional Transformer encoder, like BERT. Take text classiﬁcation as an example. We use the encoding vector of [SOS] as the representation of input, denoted as hL 1 , and feed it to a randomly initialized softmax classiﬁer (i.e., the task-speciﬁc output 1 WC), where WC ∈ Rdh×C is layer), where the class probabilities are computed as softmax(hL a parameter matrix, and C the number of categories. We maximize the likelihood of the labeled training data by updating the parameters of the pre-trained LM and the added softmax classiﬁer.',\n",
              " 'For NLG tasks, we take the sequence-to-sequence task as an example. The ﬁne-tuning procedure is similar to pre-training using the self-attention masks as in Section 2.3. Let S1 and S2 denote source and target sequences, respectively. We pack them together with special tokens, to form the input “[SOS] S1 [EOS] S2 [EOS]”. The model is ﬁne-tuned by masking some percentage of tokens in the target sequence at random, and learning to recover the masked words. The training objective is to maximize the likelihood of masked tokens given context. It is worth noting that [EOS], which marks the end of the target sequence, can also be masked during ﬁne-tuning, thus when this happens, the model learns when to emit [EOS] to terminate the generation process of the target sequence.',\n",
              " 'We have conducted experiments on both NLU (i.e., the GLUE benchmark, and extractive question answering) and NLG tasks (i.e., abstractive summarization, question generation, generative question answering, and dialog response generation).',\n",
              " 'Automatic text summarization produces a concise and ﬂuent summary conveying the key information in the input (e.g., a news article). We focus on abstractive summarization, a generation task where',\n",
              " 'Table 3: Evaluation results on CNN/DailyMail summarization. Models in the ﬁrst block are ex- tractive systems listed here for reference, while the others are abstractive models. The results of the best reported extractive model are taken from [27]. RG is short for ROUGE.',\n",
              " 'Table 4: Results on Gigaword abstractive summa- rization. Models in the ﬁrst block only use 10K examples for training, while the others use 3.8M examples. Results of OpenNMT and Transformer are taken from [4, 39]. RG is short for ROUGE.',\n",
              " 'the summary is not constrained to reusing the phrases or sentences in the input text. We use the non-anonymized version of the CNN/DailyMail dataset [37] and Gigaword [36] for model ﬁne-tuning and evaluation. We ﬁne-tune UNILM as a sequence-to-sequence model following the procedure described in Section 2.5 by concatenating document (the ﬁrst segment) and summary (the second segment) as input which is truncated according to a pre-deﬁned maximum length.',\n",
              " 'We ﬁne-tune our model on the training set for 30 epochs. We reuse most hyper-parameters from pre-training. The masking probability is 0.7. We also use label smoothing [40] with rate of 0.1. For CNN/DailyMail, we set batch size to 32, and maximum length to 768. For Gigaword, we set batch size to 64, and maximum length to 256. During decoding, we use beam search with beam size of 5. The input document is truncated to the ﬁrst 640 and 192 tokens for CNN/DailyMail and Gigaword, respectively. We remove duplicated trigrams in beam search, and tweak the maximum summary length on the development set [28, 13].',\n",
              " 'We use the F1 version of ROUGE [25] as the evaluation metric for both datasets. In Table 3, we compare UNILM against the baseline and several state-of-the-art models on CNN/DailyMail. LEAD- 3 is a baseline model that extracts the ﬁrst three sentences in a document as its summary. PGNet [37] is a sequence-to-sequence model based on the pointer-generator network. S2S-ELMo [13] uses a sequence-to-sequence model augmented with pre-trained ELMo representations, which is termed as SRC-ELMO+SHDEMB in [13]. Bottom-Up [16] is a sequence-to-sequence model augmented with a bottom-up content selector for selecting salient phrases. We also include in Table 3 the best reported extractive summarization result [27] on the dataset. As shown in Table 3, our model outperforms all previous abstractive systems, creating a new state-of-the-art abstractive summarization result on the dataset. Our model also outperforms the best extractive model [27] by 0.88 point in ROUGE-L.',\n",
              " 'In Table 4, we evaluate the models on Gigaword with different scales (10K and 3.8M). Both Transformer [43] and OpenNMT [23] implement standard attentional sequence-to-sequence models. Re3Sum [4] retrieves summaries as candidate templates, and then use an extended sequence-to- sequence model to generate summaries. MASS [39] is a pre-trained sequence-to-sequence model based on Transformer networks. Experimental results show that UNILM achieves better performance than previous work. Besides, in the low-resource setting (i.e., only 10,000 examples are used as training data), our model outperforms MASS by 7.08 point in ROUGE-L.',\n",
              " 'The task is to answer a question given a passage [33, 34, 15]. There are two settings. The ﬁrst is called extractive QA, where the answer is assumed to be a text span in the passage. The other is called generative QA, where the answer needs to be generated on the ﬂy.',\n",
              " 'Extractive QA This task can be formulated as a NLU task where we need to predict the start and end positions of the answer spans within the passage. We ﬁne-tune the pre-trained UNILM as a',\n",
              " 'Table 5: Extractive QA results on the SQuAD development set. Table 6: Extractive QA results on the CoQA development set. Table 7: Generative QA results on the CoQA development set.',\n",
              " 'bidirectional encoder for the task. We conduct experiments on the Stanford Question Answering Dataset (SQuAD) 2.0 [34], and Conversational Question Answering (CoQA) [35] datasets.',\n",
              " 'The results on SQuAD 2.0 are reported in Table 5, where we compare two models in Exact Match (EM) and F1 score. RMR+ELMo [20] is an LSTM-based question answering model augmented with pre-trained language representation. BERTLARGE is a cased model, ﬁne-tuned on the SQuAD training data for 3 epochs, with batch size 24, and maximum length 384. UNILM is ﬁne-tuned in the same way as BERTLARGE. We see that UNILM outperforms BERTLARGE.',\n",
              " 'CoQA is a conversational question answering dataset. Compared with SQuAD, CoQA has several unique characteristics. First, the examples in CoQA are conversational, so we need to answer the input question based on conversation histories. Second, the answers in CoQA can be free-form texts, including a large portion is of yes/no answers.',\n",
              " 'We modify the model used for SQuAD as follows. Firstly, in addition to the asked question, we concatenate the question-answer histories to the ﬁrst segment, so that the model can capture conversational information. Secondly, for yes/no questions, we use the ﬁnal hidden vector of the [SOS] token to predict whether the input is a yes/no question, and whether the answer is yes or no. For other examples, we select a passage subspan with the highest F1 score for training.',\n",
              " 'The results on CoQA are reported in Table 6, where we compare two models in F1 scores. DrQA+ELMo [35] is an LSTM-based question answering model augmented with pre-trained ELMo representation. BERTLARGE is a cased model, ﬁne-tuned on the CoQA training data for 2 epochs, with batch size 16, and maximum length 512. UNILM is ﬁne-tuned with the same hyper-parameters as BERTLARGE. We see that UNILM outperforms BERTLARGE.',\n",
              " 'Generative QA Generative question answering generates free-form answers for the input question and passage, which is a NLG task. In contrast, extractive methods can only predict subspans of the input passage as answers. On the CoQA dataset (as described above), Reddy et al. [2019] show that vanilla sequence-to-sequence models still underperforms extractive methods by a wide margin.',\n",
              " 'We adapt UNILM to generative question answering as a sequence-to-sequence model. The ﬁrst segment (i.e., the input sequence) is the concatenation of conversational histories, the input question and the passage. The second segment (i.e., the output sequence) is the answer. We ﬁne-tune the pre-trained UNILM on the CoQA training set for 10 epochs. We set the batch size to 32, the mask probability to 0.5, and the maximum length to 512. We also use label smoothing with rate of 0.1. The other hyper-parameters are kept the same as pre-training. During decoding, we use beam search with beam size of 3. The maximum length of input question and passage is 470. For passages that are longer than the maximum length, we split the passage into several chunks with a sliding window approach, and select a chunk with the highest word overlap over the question.',\n",
              " 'We compare our method with the generative question answering models Seq2Seq and PGNet as described in [35]. The Seq2Seq baseline is a sequence-to-sequence model with an attention mech- anism. The PGNet model augments Seq2Seq with a copy mechanism. As shown in Table 7, our generative question answering model outperforms previous generative methods by a wide margin, which signiﬁcantly closes the gap between generative method and extractive method.',\n",
              " 'We conduct experiments for the answer-aware question generation task [52]. Given an input passage and an answer span, our goal is to generate a question that asks for the answer. The SQuAD 1.1 dataset [33] is used for evaluation. Following [12], we split the original training set into training and',\n",
              " '16.38 20.76 23.75 Table 8: Question generation results on SQuAD. MTR is short for METEOR, and RG for ROUGE. Results in the groups use different data splits.',\n",
              " 'Table 9: Question generation based on UNILM improves question answering results on the SQuAD development set.',\n",
              " 'Table 10: Response generation results. Div-1 and Div-2 indicate diversity of unigrams and bigrams, respectively.',\n",
              " 'test sets, and keep the original development set. We also conduct experiments following the data split as in [51], which uses the reversed dev-test split.',\n",
              " 'The question generation task is formulated as a sequence-to-sequence problem. The ﬁrst segment is the concatenation of input passage and answer, while the second segment is the generated question.',\n",
              " 'We ﬁne-tune UNILM on the training set for 10 epochs. We set batch size to 32, masking probability to 0.7, and learning rate to 2e-5. The rate of label smoothing is 0.1. The other hyper-parameters are the same as pre-training. During decoding, we truncate the input to 464 tokens by selecting a passage chunk which contains the answer. The evaluation metrics BLEU-4, METEOR, and ROUGE-L are computed by the same scripts as in [12]. The results3 are presented in Table 8. CorefNQG [11] is based on a sequence-to-sequence model with attention and a feature-rich encoder. MP-GSN [51] uses an attention-based sequence-to-sequence model with a gated self-attention encoder. SemQG [50] uses two semantics-enhanced rewards to regularize the generation. UNILM outperforms previous models and achieves a new state-of-the-art for question generation.',\n",
              " 'Generated Questions Improve QA The question generation model can automatically harvest a large number of question-passage-answer examples from a text corpus. We show that the augmented data generated by question generation improves the question answering model.',\n",
              " 'We generate ﬁve million answerable examples, and four million unanswerable examples by modifying the answerable ones. We ﬁne-tune our question answering model on the generated data for one epoch. Then the model is ﬁne-tuned on the SQuAD 2.0 data for two more epochs.',\n",
              " 'As shown in Table 9, the augmented data generated by UNILM improves question answering model introduced in Section 3.2. Note that we use bidirectional masked language modeling as an auxiliary task for both the generated and SQuAD 2.0 datasets during ﬁne-tuning, which brings 2.3 absolute improvement compared to directly using automatically generated examples. A possible reason is that the auxiliary task alleviates catastrophic forgetting [49] when ﬁne-tuning on augmented data.',\n",
              " 'We evaluate UNILM on the document-grounded dialog response generation task [30, 15]. Given a multi-turn conversation history and a web document as the knowledge source, the system needs to',\n",
              " '3Notice that if we directly use the tokenized references provided by Du et al. [2017], the results are (21.63 BLEU-4 / 25.04 METEOR / 51.09 ROUGE-L) on the original data split [12], and (23.08 BLEU-4 / 25.57 METEOR / 52.03 ROUGE-L) in the reversed dev-test setup [51].',\n",
              " 'generate a natural language response that is both conversationally appropriate and reﬂective of the contents of the web document. We ﬁne-tune UNILM to the task as a sequence-to-sequence model. The ﬁrst segment (input sequence) is the concatenation of the web document and the conversation history. The second segment (output sequence) is the response. We ﬁne-tune UNILM on the DSTC7 training data for 20 epochs, with batch size 64. The masking probability is set to 0.5. The maximum length is 512. During decoding, we use beam search with size of 10. The maximum length of generated response is set to 40. As shown in Table 10, UNILM outperforms the best system [41] in the DSTC7 shared task [14] across all evaluation metrics.',\n",
              " 'We evaluate UNILM on the General Language Understanding Evaluation (GLUE) benchmark [45]. GLUE is a collection of nine language understanding tasks, including question answering [33], linguistic acceptability [46], sentiment analysis [38], text similarity [5], paraphrase detection [10], and natural language inference (NLI) [7, 2, 17, 3, 24, 47].',\n",
              " 'Our model is ﬁne-tuned as a bidirectional LM. We use Adamax [21] as our optimizer with a learning rate of 5e-5 and a batch size of 32. The maximum number of epochs is set to 5. A linear learning rate decay schedule with warmup of 0.1 is used. The dropout rate of the last linear projection for each task is set to 0.1, except 0.3 for MNLI and 0.05 for CoLA/SST-2. To avoid the gradient explosion issue, the gradient norm was clipped within 1. We truncated the tokens no longer than 512.',\n",
              " 'Table 11 presents the GLUE test results obtained from the benchmark evaluation server. The results show that UNILM obtains comparable performance on the GLUE tasks in comparison with BERTLARGE.',\n",
              " 'We propose a uniﬁed pre-training model, UNILM, which is jointly optimized for several LM objectives with shared parameters. The uniﬁcation of bidirectional, unidirectional, and sequence- to-sequence LMs enables us to straightforwardly ﬁne-tune the pre-trained UNILM for both NLU and NLG tasks. Experimental results demonstrate that our model compares favorably with BERT on the GLUE benchmark and two question answering datasets. In addition, UNILM outperforms previous state-of-the-art models on ﬁve NLG datasets: CNN/DailyMail and Gigaword abstractive summarization, SQuAD question generation, CoQA generative question answering, and DSTC7 dialog response generation.',\n",
              " 'The work can be advanced from the following perspectives:',\n",
              " 'Acknowledgement We would like to acknowledge Shiyue Zhang for the helpful discussions about the question generation experiments.',\n",
              " 'Our model can generate text samples using the left-to-right setting. We picked three text samples sampled from left to right using our model, as shown in Table 12. We use the top-40 truncating sampling strategy [32], and forbid duplicate 4-grams during generation. For each example, we sampled 10 times from the same input and we hand-picked the best one; as such, these samples should be considered to be better than the average model output. From the examples, we ﬁnd that the model can produce ﬂuent output with somewhat consistent contents which ﬁts the inputs’ genres and topics. In the ﬁrst example, given a modiﬁed excerpt from the novel “1984” as input, the model’s output is like a short paragraph in an fantasy novel; for the other two with input from Wikipedia, the model makes up “facts” with plausible names, time stamps and events written in the Wikipedia style.',\n",
              " 'As shown in Table 13, we summarize the data size and the evaluation metrics used for the General Language Understanding Evaluation (GLUE) benchmark.',\n",
              " 'Maurice Ravel (7 March 1875 - 28 December 1937) was a French composer, pianist and conductor. He was born in Paris, and studied piano with Eugene Massenet. In the early 1900s Ravel became interested in the composition of music, and he composed much of his early music for the piano. Ravel’s early work is marked by a marked sense of lyricism, but in the later years of his career he developed a strong preference for orchestral forms. His ﬁrst work, “le Grand Cours d’ Auvergne”, was composed in 1909 by Maurice Ravel, a student of Jules Massenet, and was published by A.F.A. in 1912. It was re-published in 1912 by the publisher J.S.D.M. de l’Etablissement Musicale de la Musique Francaise. Ravel wrote the piano concerto “la Tragedie et la Chanson Dans le Theatre des Champs Elysees” in 1916. (...)',\n",
              " 'Table 12: Text samples generated by our model using left-to-right generation.']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ListItem=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
        "            ListItem.append(str(element))"
      ],
      "metadata": {
        "id": "WCOL1yxrjm7F"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ListItem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yhukmnvjsFj",
        "outputId": "dc9a885b-0015-490c-8e57-aaa8783ca1e0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['• We will push the limit of the current method by training more epochs and larger models on web- scale text corpora. At the same time, we will also conduct more experiments on end applications as well as ablation experiments to investigate the model capability and the beneﬁts of pre-training multiple language modeling tasks with the same network.',\n",
              " '• We are focusing on monolingual NLP tasks in our current experiments. We are also interested in extending UNILM to support cross-lingual tasks [6].',\n",
              " '• We will conduct multi-task ﬁne-tuning on both NLU and NLG tasks, which is a natural extension of Multi-Task Deep Neural Network (MT-DNN) [26].',\n",
              " '[1] Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven pretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019.',\n",
              " '[2] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second PASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 01 2006.',\n",
              " '[3] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The ﬁfth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC-09), 2009.',\n",
              " '[4] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template based neural summarization. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 152–161, Melbourne, Australia, July 2018. Association for Computational Linguistics.',\n",
              " '[5] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.',\n",
              " '[6] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang. Cross- lingual natural language generation via pre-training. ArXiv, abs/1909.10481, 2019.',\n",
              " '[7] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entail- ment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classiﬁcation, and Recognizing Textual Entailment, MLCW’05, pages 177–190, Berlin, Heidelberg, 2006. Springer-Verlag.',\n",
              " '[8] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems 28, pages 3079–3087. Curran Associates, Inc., 2015.',\n",
              " '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.',\n",
              " '[10] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential para- phrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.',\n",
              " '[11] Xinya Du and Claire Cardie. Harvesting paragraph-level question-answer pairs from Wikipedia. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 1907–1917, Melbourne, Australia, July 2018. Association for Computational Linguistics.',\n",
              " '[12] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1342–1352, 2017.',\n",
              " '[13] Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations for language generation. CoRR, abs/1903.09722, 2019.',\n",
              " '[14] Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. Grounded response generation task at dstc7. In AAAI Dialog System Technology Challenges Workshop, 2019.',\n",
              " '[15] Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Founda- tions and Trends in Information Retrieval, 13(2-3):127–298, 2019.',\n",
              " '[16] Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098–4109, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.',\n",
              " '[17] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL In Proceedings of the ACL-PASCAL Workshop recognizing textual entailment challenge. on Textual Entailment and Paraphrasing, pages 1–9, Prague, June 2007. Association for Computational Linguistics.',\n",
              " '[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016.',\n",
              " '[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi- ﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational Linguistics.',\n",
              " '[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify: Machine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018.',\n",
              " '[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.',\n",
              " '[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, San Diego, CA, 2015.',\n",
              " '[23] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT: In Proceedings of ACL 2017, System Open-source toolkit for neural machine translation. Demonstrations, pages 67–72, Vancouver, Canada, July 2017. Association for Computational Linguistics.',\n",
              " '[24] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.',\n",
              " '[25] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza- tion Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.',\n",
              " '[26] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. CoRR, abs/1901.11504, 2019.',\n",
              " '[27] Yang Liu. Fine-tune BERT for extractive summarization. CoRR, abs/1903.10318, 2019.',\n",
              " '[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. CoRR, abs/1705.04304, 2018.',\n",
              " '[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.',\n",
              " '[30] Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin Choi, and Jianfeng Gao. Conversing by reading: Contentful neural conversation with on- In Proceedings of the 57th Annual Meeting of the Association demand machine reading. for Computational Linguistics, pages 5427–5436, Florence, Italy, July 2019. Association for Computational Linguistics.',\n",
              " '[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.',\n",
              " '[32] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.',\n",
              " '[33] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ ques- tions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics.',\n",
              " '[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable In Proceedings of the 56th Annual Meeting of the Association for questions for SQuAD. Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, pages 784–789, 2018.',\n",
              " '[35] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249–266, March 2019.',\n",
              " '[36] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal, September 2015. Association for Computational Linguistics.',\n",
              " '[37] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1073–1083, Vancouver, Canada, July 2017. Association for Computational Linguistics.',\n",
              " '[38] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642, 2013.',\n",
              " '[39] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.',\n",
              " '[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re- thinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818–2826, 2016.',\n",
              " '[41] Y Tam, Jiachen Ding, Cheng Niu, and Jie Zhou. Cluster-based beam search for pointer-generator chatbot grounded by knowledge. In AAAI Dialog System Technology Challenges Workshop, 2019.',\n",
              " '[42] Wilson L Taylor. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415–433, 1953.',\n",
              " '[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017.',\n",
              " '[44] Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov random ﬁeld language model. CoRR, abs/1902.04094, 2019.',\n",
              " '[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019.',\n",
              " '[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.',\n",
              " '[47] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.',\n",
              " '[48] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016.',\n",
              " '[49] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and arXiv preprint Phil Blunsom. Learning and evaluating general linguistic intelligence. arXiv:1901.11373, 2019.',\n",
              " '[50] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi- supervised question answering. CoRR, abs/1909.06356, 2019.',\n",
              " '[51] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question generation with maxout pointer and gated self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3901–3910, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.',\n",
              " '[52] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural question generation from text: A preliminary study. In Xuanjing Huang, Jing Jiang, Dongyan Zhao, Yansong Feng, and Yu Hong, editors, Natural Language Processing and Chinese Computing, pages 662–671. Springer International Publishing, 2018.',\n",
              " '[53] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE International Conference on Computer Vision, pages 19–27, 2015.']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C-cLs47juJf",
        "outputId": "d857c63c-6698-480d-9a94-a3554f3a5497"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.2.22-py3-none-any.whl (373 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/373.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/373.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m368.6/373.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain_core)\n",
            "  Downloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.5.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2024.7.4)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain_core\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain_core-0.2.22 langsmith-0.1.93 orjson-3.10.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AsnYdhtkfta",
        "outputId": "201d58c8-374f-4d89-d1f1-e694b9b45962"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.17-py3-none-any.whl (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3.0,>=0.2.20 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.22)\n",
            "Collecting openai<2.0.0,>=1.32.0 (from langchain_openai)\n",
            "  Downloading openai-1.36.1-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.8/328.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain_openai) (0.1.93)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain_openai) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.20->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.20->langchain_openai) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.20->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.20->langchain_openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Installing collected packages: tiktoken, openai, langchain_openai\n",
            "Successfully installed langchain_openai-0.1.17 openai-1.36.1 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P732Tjxy-3p",
        "outputId": "72b55077-0d12-4ba4-ae8d-0c6fa7a64b73"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tab[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "nU7Dl1R1zBLB",
        "outputId": "c25e5af4-ddfe-4a0f-e5f0-3dc26404cca9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ELMo GPT BERT UNILM Left-to-Right LM v v Right-to-Left LM v Bidirectional LM v Sequence-to-Sequence LM v v v'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yafCoI7VzMcC",
        "outputId": "69e0a0a9-cfe8-455f-c623-d29ec36633c2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "PF-wtZhJzOG7"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw table elements. \\\n",
        "    Give a concise summary of the table that is well optimized for retrieval. Table {element} \"\"\""
      ],
      "metadata": {
        "id": "o-jqaiZ1zQSZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(prompt_text)"
      ],
      "metadata": {
        "id": "bO7DBkbuzToy"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "OPENAI_API_TOKEN=userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
      ],
      "metadata": {
        "id": "s_evhXZBzblw"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text summary chain\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "9kg2TEaWzVM2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "RpPqWW13zlCH"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries = []"
      ],
      "metadata": {
        "id": "Vid7dc6GznX1"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries=summarize_chain.batch(tab,{\"max_concurrency\": 5})"
      ],
      "metadata": {
        "id": "UYqZewpwzpGH"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab[0]"
      ],
      "metadata": {
        "id": "Y1CWCsJUzs9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries[0]"
      ],
      "metadata": {
        "id": "lvA_3m1Zz50P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img[0]"
      ],
      "metadata": {
        "id": "2-b-lFFK0AIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import os\n",
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "dOZn3wwrz80C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path):\n",
        "    \"\"\"Getting the base64 string\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "pCCmxo8J0TtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_summarize(img_base64, prompt):\n",
        "    \"\"\"Make image summary\"\"\"\n",
        "\n",
        "\n",
        "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
        "\n",
        "    msg = chat.invoke(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "\n",
        "                     {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return msg.content"
      ],
      "metadata": {
        "id": "xkpkeneW0VWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_img_summaries(path):\n",
        "    \"\"\"\n",
        "    Generate summaries and base64 encoded strings for images\n",
        "    path: Path to list of .jpg files extracted by Unstructured\n",
        "    \"\"\"\n",
        "\n",
        "    # Store base64 encoded images\n",
        "    img_base64_list = []\n",
        "\n",
        "    # Store image summaries\n",
        "    image_summaries = []\n",
        "\n",
        "    # Prompt\n",
        "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw image. \\\n",
        "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
        "\n",
        "\n",
        "    base64_image = encode_image(path)\n",
        "    img_base64_list.append(base64_image)\n",
        "    image_summaries.append(image_summarize(base64_image, prompt))\n",
        "\n",
        "    return img_base64_list, image_summaries\n",
        ""
      ],
      "metadata": {
        "id": "Jgnn34xk0dNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpath=\"/content/extracted_data2/figure-17-4.jpg\""
      ],
      "metadata": {
        "id": "D9ttyy490d-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_base64_list,image_summaries=generate_img_summaries(fpath)"
      ],
      "metadata": {
        "id": "6hfw3y440eCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_summaries[0]"
      ],
      "metadata": {
        "id": "rT8lXig10lMM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}